<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Data Traceability</title>
    <meta name="author" content="Reid Draper" />

    <link href="http://feeds.feedburner.com/reiddraper" rel="alternate" title="Reid Draper" type="application/atom+xml" />

    <link rel="stylesheet" href="../css/site.css" type="text/css" media="screen, projection" />

    <link rel="stylesheet" href="https://use.typekit.net/cpj3him.css" />
    <script type="text/javascript">
      try {
        Typekit.load();
      } catch (e) {}
    </script>
    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(["_setAccount", "UA-15201954-3"]);
      _gaq.push(["_trackPageview"]);

      (function () {
        var ga = document.createElement("script");
        ga.type = "text/javascript";
        ga.async = true;
        ga.src =
          ("https:" == document.location.protocol
            ? "https://ssl"
            : "http://www") + ".google-analytics.com/ga.js";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(ga, s);
      })();
    </script>
  </head>
  <body>
    <div class="container">
      <a id="site-title" href="../">reid draper</a>

      <article class="content"><h1>Data Traceability</h1>
<h3>May 16, 2013</h3>
<p>This text appears as Chapter 17 in O’Reilly’s <a href="http://shop.oreilly.com/product/0636920024422.do">Bad Data Handbook</a> (ISBN-13: 978-1449321888). It is released under the <a href="http://creativecommons.org/licenses/by-sa/3.0/">CC BY-SA</a> license.</p>
<hr />
<p>Your software consistently provides impressive music recommendations by combining cultural and audio data. Customers are happy. However, things aren’t always perfect. Sometimes that Beyoncé track is attributed to Beyonce. The artist for the Béla Fleck solo album shows up as Béla Fleck and the Flecktones. Worse, the ボリス biography has the artist name listed as ???. Where did things go wrong? Did one of your customers provide you with data in an incorrect character encoding? Did one of the web-crawlers have a bug? Perhaps the name resolution code was incorrectly combining a solo artist with his band?</p>
<p>How do we solve this problem? We’d like to be able to trace data back to it’s origin, following each transformation. This is reified as <em>data provenenace</em>. In this chapter, we’ll explore ways of keeping track of the source of our data, techniques for backing out bad data, and the business value of adopting such ability.</p>
<h2 id="why">Why?</h2>
<p>The ability to trace a datum back to its origin is important for several reasons. It helps us to back-out or reprocess bad data, and conversely, it allows us to reward and boost good data sources and processing techniques. Furthermore, local privacy laws can mandate things like auditability, data transfer restrictions and more. For example, California’s Shine the Light Law requires businesses disclose the personal information that has been shared with third-parties, should a resident request. Europe’s Data Protection Directive provides even more stringent regulation to businesses collecting data about residents.</p>
<p>We’ll also later see how data traceability can provide further business value by allowing us to provide stronger measurements on the worth of a particular source, realize where to focus our development effort, and even manage blame.</p>
<h2 id="personal-experience">Personal Experience</h2>
<p>I previously worked in the data ingestion team at a music data company. We provided artist and song recommendations, artist biographies, news, and detailed audio analysis of digital music. We exposed those data feeds via web services and raw dumps. Behind the scenes, these feeds were composed of many sources of data, which which were in turn cleaned, transformed, and put through machine learning algorithms.</p>
<p>One of the first issues we ran into was learning how to trace a particular result back to its constituent parts. If a given artist recommendation was poor, was it because of our machine learning algorithm? Did we simply not have enough data for that artist? Was there some obviously wrong data from one of our sources? Being able to debug our product became a business necessity.</p>
<p>We developed several mechanisms for being able to debug our data woes, some of which I’ll explore here.</p>
<h3 id="snapshotting">Snapshotting</h3>
<p>Many of the data sources were updated frequently. At the same time, the web pages we crawled for news, reviews, biography information and similarity, were updated inconsistently. This meant that even if we were able to trace a particular datum back to its source, that source may have been drastically different than the time we had previously crawled or processed the data. In turn, we needed to not only capture the source of our data, but the time, and exact copy of the source. Our database columns or keys would then have an extra field for a timestamp.</p>
<p>Keeping track of the time and the original data also allows you to track changes from that source. You get closer to answering the question, “why were my recommendations for The Sea and Cake great last week, but terrible today?”</p>
<p>This process of writing data once and never changing it is called <em>immutability,</em> and it plays a key role in data traceability. I’ll return to it later, when I walk through an example.</p>
<h3 id="saving-the-source">Saving the source</h3>
<p>Our data was stored in several different types of databases, including relational and key-value stores. However, nearly every schema had a <em>source</em> field. This field would contain one or more values. For original sources there would be a single source listed. As data was processed and transformed into roll-ups or learned-data, we would preserve the list of sources that went into creating that new piece of data. This allowed us to trace the final data product back to its constituent parts.</p>
<h3 id="weighting-sources">Weighting sources</h3>
<p>One of the most important reason we collected data was to learn about new artists, albums and songs. That said, we didn’t always want to create a new entity that would end up in our final data product. Certain data sources were more likely to have errors, misspellings and other inaccuracies, so we wanted them to be vetted before they would progress through our system.</p>
<p>Furthermore, we wanted to be able to give priority processing to certain sources that either had higher information value or were for a particular customer. For applications like learning about new artists, we’d assign a trust-score to each source that would, among other things, determine whether a new artist was created.</p>
<p>If the artist wasn’t created based solely on this source, it would add weight to that artist being created if we ever heard of them again. In this way, the combined strength of several lower-weighted sources could lead the artist being created in our application.</p>
<h3 id="backing-out-data">Backing out data</h3>
<p>Sometimes we identified that data was simply incorrect or otherwise bad. In such cases, we had to both remove the data from our production offering.</p>
<p>Recall, our data would pass through several stages of transformation on its way to the production offering. A backout, then, required that we first identify potential sources of the bad data, remove it, then reprocess the product without that source. (Sometimes the data transformations were so complex that it was easier to generate all permutations of source data, to spot the offender.) This is only possible since we had kept track of the sources that went into the final product.</p>
<p>Because of this observation, we had to make it easy to redo any stage of the data transformation with an altered source list. We designed our data processing pipeline to use parameterized source lists, so that it was easy to exclude a particular source, or explicitly declare the sources that were allowed to affect this particular processing stage.</p>
<h3 id="separating-phases-and-keeping-them-pure">Separating phases (and keeping them pure)</h3>
<p>Often we would divide our data processing into several stages. It’s important to identify the state barriers in your application, as doing this allowed us to both write better code, and create more efficient infrastructure.</p>
<p>From a code perspective, keeping each of our stages separate allowed us to reduce side effects (such as I/O). In turn, this made code easier to test, because we didn’t have to set up mocks for half of our side-effecting infrastructure.</p>
<p>From an infrastructure perspective, keeping things separate allowed us to make isolated decisions about each stage of the process, ranging from compute power, to parallelism, to memory constraints.</p>
<h3 id="identifying-the-root-cause">Identifying the root cause</h3>
<p>Identifying the root cause of data issues is important to being able to fix them, and control customer relationships. For instance, if a particular customer is having a data quality issue, it is helpful to know whether the origin of the issue was from data they gave you, or from your processing of the data they gave you. In the former case, there is real business value in being able to show the customer the exact source of the issue, as well as your solution.</p>
<h3 id="finding-areas-for-improvement">Finding areas for improvement</h3>
<p>Related to blame is the ability to find sources of improvement in your own processing pipeline and infrastructure. This means the steps in your processing pipeline become data sources in their own right.</p>
<p>It’s useful to know, for instance, when and how you derived a certain piece of data. Should an issue arise, you can immediately focus on the place it was created. Conversely, if a particular processing stage tends to produce excellent results, it is helpful to be able to understand why that is so. Ideally you can then replicate this into other parts of your system.</p>
<p>Organizationally, this type of knowledge also allows you to determine where to focus your teams’ effort, and even to reorganize your team structure. For example, you might want to place a new member of the team on one of the infrastructure pieces that is doing well, and should be a model for other pieces, as to give them a good starting place for learning the system. A more senior team member may be more effective on pieces of the infrastructure that are struggling.</p>
<h2 id="immutability-borrowing-an-idea-from-functional-programming">Immutability: borrowing an idea from functional programming</h2>
<p>Considering the examples above, a core element of our strategy was <em>immutability</em>: even though our processing pipeline transformed our data several times over, we never changed (overwrote) the original data.</p>
<p>This is an idea we borrowed from functional programming. Consider imperative languages like C, Java and Python, in which data tends to be mutable. For example, if we want to sort a list, we might call <code>myList.sort()</code>. This will sort the list in-place. Consequently, all references to <code>myList</code> will be changed. If we now want review <code>myList</code>’s original state, we’re out of luck: we should have made a copy before calling <code>sort()</code>.</p>
<p>By comparison, functional languages like Haskell, Clojure and Erlang tend to treat data as immutable. Our list sorting example becomes something closer to <code>myNewSortedList = sort(myList)</code>. This retains the unsorted list <code>myList</code>. One of the advantages of this immutability is that many functions become simply the result of processing the values passed in. Given a stack trace, we can often reproduce bugs immediately.</p>
<p>With mutable data, there is no guarantee that the value of a particular variable remains the same throughout the execution of the function. Because of this, we can’t necessarily rely on a stack trace to reproduce bugs.</p>
<p>Concerning our data processing pipeline, we could save each step of transformation and debug it later. For example, consider this workflow:</p>
<pre><code>rawData = downloadFrom(someSite)
cleanData = cleanup(rawData)
newArtistData = extractNewArtists(cleanData)</code></pre>
<p>Let’s say we’ve uncovered a problem in the <code>cleanup()</code> function. We would only have to correct the code and rerun that stage of the pipeline. We never replaced <code>rawData</code> and hence it would be available for any such debugging later.</p>
<p>To take further advantage of immutability, we persisted our data under a compound key of identifier and timestamp. This helped us find the exact inputs to any of our data processing steps, which saved time when we had to debug an issue.</p>
<h2 id="an-example">An Example</h2>
<p>As an example, let me walk you through creating a news aggregation site. Along the way, I’ll apply the lessons I describe above to demonstrate how data traceability affects the various aspects of the application.</p>
<p>Let’s say that our plan is to display the top stories of the day, with the ability to drill down by topic. Each story will also have a link to display coverage of the same event from other sources.</p>
<p>We’ll need to be able to do several things:</p>
<ol type="1">
<li>Crawl the web for news stories.</li>
<li>Determine a story’s popularity and timeliness based on social media activity, and perhaps its source. (For example, we assume a story on the New York Times homepage is important and/or popular).</li>
<li>Cluster stories about the same event together.</li>
<li>Determine event popularity. (Maybe this will be aggregate popularity of the individual stories?)</li>
</ol>
<h3 id="crawlers">Crawlers</h3>
<p>We’ll seed our crawlers with a number of known news sites. Every so often we’ll download the contents of the page and store it under a composite key with URL, source and timestamp, or a relational database row with these attributes. (Let’s say we crawl frequently-updated pages several times a day, and just once a day for other pages.)</p>
<p>From each of these home pages we crawl, we’ll download the individual linked stories. The stories will also be saved with URL, source and timestamp attributes. Additionally, we’ll store the composite ID of the homepage where we were linked to this story. That way if, for example, later we suspect we have a bug with the way we assign story popularity based on home page placement, we can review the home page as it was retrieved at a particular point in time. Ideally we should be able to trace data from our own homepage all the way back to the original HTML that our crawler downloaded.</p>
<p>In order to help determine popularity, and to further feed our news crawlers, we’ll also crawl social media sites. Just like with the news crawlers, we’ll want to keep a timestamped record of the HTML and other assets we crawl. Again, this will let us go back later and debug our code. One example of why this would be useful is if we suspect we are incorrectly counting links from shares of a particular article.</p>
<h3 id="change">Change</h3>
<p>Keeping previous versions of the sites we crawl allows for some interesting analytics. Historically, how many articles does the Boston Globe usually link to on their home page? Is there a larger variety of news articles in the summer? Another useful byproduct of this is that we can run new analytics on past data. Because immutability can give us a basis from the past, we’re not confined to just the data we’ve collected since we turned on our new analytics.</p>
<h3 id="clustering">Clustering</h3>
<p>Clustering data is a difficult problem. Outlying or mislabeled data can completely change our clusters. For this reason, it is important to be able to cheaply (in human and compute time) be able to experiment with rerunning our clustering with altered inputs. The inputs we alter may remove data from a particular source, or add a new topic modelling stage between crawling and clustering. In order to achieve this, our infrastructure must be loosely coupled such that we can just as easily provide inputs to our clustering system for testing as we do in production.</p>
<h3 id="popularity">Popularity</h3>
<p>Calculating story popularity shares many of the same issues as clustering stories. As we experiment, or debug an issue, we want to quickly test our changes and see the result. We also want to see the most popular story on our own page and trace all the way through our own processing steps, back to the origin site we crawled. If we find out we’ve ranked a story as more popular that we would’ve liked, we can trace it back to our origin crawl to see if, perhaps, we had put too much weight in its position on its source site.</p>
<h2 id="conclusion">Conclusion</h2>
<p>You will need to debug data processing code and infrastructure just like normal code. By taking advantage of techniques like immutability, you can dramatically improve your ability to reason about your system. Furthermore, we can draw from decades of experience in software design to influence our data processing and infrastructure decisions.</p>
<p class="twitter_follow">If you like this post, you should follow me on
  <a href="http://twitter.com/reiddraper">twitter.</a>
</p>
</article>

      <div id="footer">
        <span id="twitter">
          <a href="http://twitter.com/reiddraper">@reiddraper</a>
        </span>
        |
        <span id="github">
          <a href="https://github.com/reiddraper">github</a>
        </span>
      </div>
    </div>
  </body>
</html>
